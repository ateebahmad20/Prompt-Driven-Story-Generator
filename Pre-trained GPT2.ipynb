{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Lab 11 - Story Generation From a Given Prompt Via Transformers\n",
        "# Ateeb Ahmad\n",
        "# 334030"
      ],
      "metadata": {
        "id": "HBp9Bijbl-ZL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWgzTH90ieD7"
      },
      "source": [
        "##Uploading Kaggle Json file to Use Kaggle Dataset Download API Command\n",
        "You can get yours from Kaggle->Profile->Account->API->Create New Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 96
        },
        "id": "lSvgsx6meXlT",
        "outputId": "a732ba83-8b57-427c-b28e-1f2ac550a6c0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bcca35d6-5fd4-4b93-8a25-3ce4a767b1c7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bcca35d6-5fd4-4b93-8a25-3ce4a767b1c7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"ateebahmad\",\"key\":\"f5df25d3e20074fa692b59d153b472a6\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLifNfMAjBbK"
      },
      "source": [
        "##Moving the Json file to Kaggle folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "a9CCCrcLetYR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "192fb7f6-c385-4d23-d182-e9c80fb1424f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/root/.kaggle': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rm -r ~/.kaggle # Removing any existing kaggle directory\n",
        "!mkdir ~/.kaggle # Creating new directory\n",
        "!mv ./kaggle.json ~/.kaggle/ # Moving the Json file to Kaggle directory\n",
        "!chmod 600 ~/.kaggle/kaggle.json # Changing the permission to only owner can access and read it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4ug7J41jkBq"
      },
      "source": [
        "##Downloading the Dataset from Kaggle and Unzipping it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmKPuCETlKzE",
        "outputId": "40be0dbc-458d-44ac-8db9-da76da345cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading writing-prompts.zip to /content\n",
            "100% 369M/370M [00:10<00:00, 42.4MB/s]\n",
            "100% 370M/370M [00:10<00:00, 38.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d ratthachat/writing-prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_Z63YPrLfp2r"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip /content/writing-prompts.zip;"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "dfGMCmbHo9t6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import math\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "zk_9cUicpAmF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using PreTrained GPT Model & Tokenizer"
      ],
      "metadata": {
        "id": "RyPc1CejhLyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2', pad_token='pad')\n",
        "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "EoeOkj2qWB1o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f73d8d3-89d3-4708-ec24-0e4e92dbfe39"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating Dataset"
      ],
      "metadata": {
        "id": "o-C8jRV5TDyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(Dataset):\n",
        "  def __init__(self, prompt, story):\n",
        "\n",
        "    # Opening the respective source and target files line-by-line\n",
        "    self.prompt = open(prompt).readlines()\n",
        "    self.stories = open(story).readlines()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.prompt)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    # Returning the prompt and it's respective story\n",
        "    return self.prompt[index].strip(), self.stories[index].strip()"
      ],
      "metadata": {
        "id": "WkbtMZ_UTly1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = Dataset(\"/content/writingPrompts/train.wp_source\", \"/content/writingPrompts/train.wp_target\")\n",
        "val_set = Dataset(\"/content/writingPrompts/valid.wp_source\", \"/content/writingPrompts/valid.wp_target\")\n",
        "test_set = Dataset(\"/content/writingPrompts/test.wp_source\", \"/content/writingPrompts/test.wp_target\")"
      ],
      "metadata": {
        "id": "kbj8rkguVnrC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Showing Data"
      ],
      "metadata": {
        "id": "R7syB3l1dVdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i,j in val_set:\n",
        "  print(\"prompt: \", i)\n",
        "  print(\"story: \", j)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5fg_j3YdBlQ",
        "outputId": "666c46ed-66fb-4466-edbb-a39ece050758"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt:  [ WP ] Every person in the world undergoes a `` goodness '' test . It 's designed to give a score from 1 to 200 , where 1 is pure evil , and 200 is an angel in human body . Then the world is divided into 200 zones , where people can live among their own kind .\n",
            "story:  Clancy Marguerian , 154 , private first class of the 150+ army , sits in his foxhole . Tired cold , wet and hungry , the only thing preventing him from laying down his rifle and walking towards the enemy lines in surrender is the knowledge that however bad he has it here , life as a 50-100 POW is surely much worse . He 's fighting to keep his eyes open and his rifle ready when the mortar shells start landing near him . <newline> <newline> He hunkers lower . <newline> <newline> After a few minutes under the barrage , Marguerian hears hurried footsteps , a grunt , and a thud as a soldier leaps into the foxhole . The man 's uniform is tan , he must be a 50-100 . <newline> <newline> The two men snarl and grab at eachother , grappling in the small foxhole . Abruptly , their faces come together . <newline> <newline> `` Clancy ? '' <newline> <newline> `` Rob ? '' <newline> <newline> Rob Hall , 97 , Corporal in the 50-100 army grins , as the situation turns from life or death struggle , to a meeting of two college friends . He lets go of Marguerian 's collar . <newline> <newline> `` Holy shit Clancy , you 're the last person I expected to see here '' <newline> <newline> `` Yeah '' <newline> <newline> `` Shit man , I did n't think I 'd ever see 'Mr . volunteers every saturday morning at the food shelf ' , not after The Reorganization at least '' <newline> <newline> `` Yeah Rob , it is something is n't it '' <newline> <newline> `` Man , I 'm sorry I tried to kill you there , hey , I heard you guys were out of food , here , you can share my dinner '' <newline> <newline> Clancy marvels , even after all this : The Reorganization , the coalitions , the war , Rob is still his old , chatty self . <newline> <newline> The two men sit , Rob chatting away , Clancy forcing out pleasantries . They pass Rob 's rations between them . <newline> <newline> <newline> `` Clancy my man , I heard a group of terrorist 5 's took have formed some kind of cult , and they 're rallying all the < 50 in their own coalition '' <newline> <newline> `` Oh yeah ? '' <newline> <newline> `` Yeah , I mean , that sucks and everything , cause those are some scary dudes , but I heard that there 's going to be a truce between our countries in a few days , why do n't we just hang out here , pretty soon we wo n't even be enemies anymore ! '' <newline> <newline> `` Yeah , Rob , that sounds like a plan '' <newline> <newline> `` Man , I 'm so glad I found you again , in a few days , this war will be over , and things will be cool between us and , hey , remember Sarah ? I heard she 's a 151 , maybe I 'll look her up , I 'll be sure to visit you too once I can get a pass to sector 150-155 , it 'll probably be tough though , even before the war , you had to do sooo much paperwork to be allowed to visit , I wonder if passes will even be reinstated after the truce ends , hey , did I ever tell you about the time ... '' <newline> <newline> Rob babbles as he dozes off , grinning up at Clancy . <newline> <newline> When Clancy is sure that his friend is asleep , he slits Rob 's throat with his bayonet . Clancy climbs out of the foxhole , and stumbles his way back to battalion HQ .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating DataLoaders"
      ],
      "metadata": {
        "id": "eGcEmlQEhHCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batchSize = 6\n",
        "\n",
        "trainLoader = DataLoader(\n",
        "    train_set,\n",
        "    batch_size = batchSize,\n",
        "    shuffle = True,\n",
        ")\n",
        "\n",
        "valLoader = DataLoader(\n",
        "    val_set,\n",
        "    batch_size = batchSize,\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "testLoader = DataLoader(\n",
        "    test_set,\n",
        "    batch_size = 4,\n",
        "    shuffle = False\n",
        ")"
      ],
      "metadata": {
        "id": "1QwBT-j-CsYx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defining Loss Function and Optimizer"
      ],
      "metadata": {
        "id": "lTg2L1S4mE7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Scaler\n",
        "scaler = GradScaler()"
      ],
      "metadata": {
        "id": "fVCg1nDumIq-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defining Training & Testing Function"
      ],
      "metadata": {
        "id": "0-IT96Q_hron"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IYkNVtic-hrZ"
      },
      "outputs": [],
      "source": [
        "def train_or_val(model, optimizer, dataloader, train):\n",
        "\n",
        "  if train:\n",
        "    model.train()\n",
        "  else:\n",
        "    model.eval()\n",
        "\n",
        "  running_loss = 0\n",
        "\n",
        "  for prompt, stories in tqdm(dataloader):\n",
        "\n",
        "    # Passing Inputs to Tokenizer\n",
        "    input = tokenizer(prompt, return_tensors='pt', truncation=True, padding='max_length', max_length = 1000)['input_ids']\n",
        "    labels = tokenizer(stories, return_tensors='pt', truncation=True, padding='max_length',  max_length = 1000)['input_ids']\n",
        "\n",
        "    # Passing them on CUDA or CPU depending on the availability\n",
        "    input = input.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Forward Pass with autocasting\n",
        "    with autocast():\n",
        "      yhat = model(input_ids = input, labels = labels)\n",
        "\n",
        "    # Computing loss\n",
        "    loss = yhat.loss\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    # Backpropogation\n",
        "    if train:\n",
        "      optimizer.zero_grad() # Making gradient equal to zero to avoid accumulation\n",
        "      scaler.scale(loss).backward() # Calculating Gradients\n",
        "      scaler.step(optimizer) # Updating weights and biases\n",
        "      scaler.update() # Updating the Scaled Loss\n",
        "\n",
        "  # Calculating Loss\n",
        "  loss = running_loss/len(dataloader)\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eBGA3hLKwWQ"
      },
      "source": [
        "##Mounting Google Drive for Ease of Downloading & Uploading Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yDKrngz4K0ep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de2a0ac5-aa5d-40da-85d1-3cefc4b05a27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzfsQEFGAzhr"
      },
      "source": [
        "##Defining Paths for Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vY8OKpL2A2sX"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = '/content/drive/MyDrive/DL_Lab11/checkpoint.pth'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Starting Training"
      ],
      "metadata": {
        "id": "EXHyVjZXhxvX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STP_GQeyZwkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34bb8916-206a-431e-932d-4ebfe0c4b917"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2604/2604 [24:21<00:00,  1.78it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 4.856\n",
            "Perplexity Score: 128.51598851799886\n",
            "\n",
            "Epoch 2: Train\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2604/2604 [24:25<00:00,  1.78it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 4.757\n",
            "Perplexity Score: 116.3992772716706\n",
            "\n",
            "Epoch 3: Train\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2604/2604 [24:15<00:00,  1.79it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 4.723\n",
            "Perplexity Score: 112.5125796693891\n",
            "\n",
            "Epoch 4: Train\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2604/2604 [24:19<00:00,  1.78it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 4.696\n",
            "Perplexity Score: 109.5618079215732\n",
            "\n",
            "Epoch 5: Train\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2604/2604 [24:12<00:00,  1.79it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 4.673\n",
            "Perplexity Score: 107.03646963227852\n",
            "\n",
            "Epoch 6: Train\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2604/2604 [24:13<00:00,  1.79it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss: 4.653\n",
            "Perplexity Score: 104.92211479934501\n",
            "\n",
            "Epoch 7: Train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 77%|███████▋  | 2013/2604 [18:38<05:25,  1.82it/s]"
          ]
        }
      ],
      "source": [
        "Epochs = 10\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for i in range(0, Epochs):\n",
        "\n",
        "  print(f\"Epoch {i + 1}: Train\")\n",
        "  t_loss = train_or_val(model, optimizer, valLoader, True)\n",
        "  train_losses.append(t_loss)\n",
        "\n",
        "  print(f\"\\nTrain Loss: {t_loss:>0.3f}\")\n",
        "  print(f\"Perplexity Score: {math.exp(t_loss)}\\n\")\n",
        "\n",
        "  checkpoint = {\n",
        "      \"model\": model.state_dict(),\n",
        "      \"optimizer\": optimizer.state_dict()\n",
        "      }\n",
        "\n",
        "  # Saving the Checkpoint after each Epoch\n",
        "  torch.save(checkpoint, checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing"
      ],
      "metadata": {
        "id": "ioPwTSkq_VZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Checkpoint\n",
        "checkpoint = torch.load(checkpoint_path, map_location=torch.device(device))\n",
        "\n",
        "# Loading the state of model\n",
        "model.load_state_dict(checkpoint['model'])"
      ],
      "metadata": {
        "id": "mYsTvcKCACTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4568718f-2fb1-4ad5-e6d2-ca23aa342390"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = train_or_val(model, optimizer, testLoader, False)\n",
        "print(f\"\\nTrain Loss: {loss:>0.3f}\")\n",
        "print(f\"Perplexity Score: {math.exp(loss)}\\n\")"
      ],
      "metadata": {
        "id": "LVaejbrm_XnC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad55722-6a68-431c-cc94-da36129cb20c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3785/3785 [09:51<00:00,  6.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 4.907\n",
            "Perplexity Score: 135.20822263553882\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inferencing Model"
      ],
      "metadata": {
        "id": "lRNUcU98_6g0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxbrHpNy-yu_",
        "outputId": "9b7b6d4f-c8d9-4143-f28e-40510b9622b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "prompt :\n",
            "You've finally managed to discover the secret to immortality. Suddenly, Death appears before you, hands you a business card, and says, `` When you realize living forever sucks, call this number, I've got a job offer for you. ''\n",
            "story:\n",
            "`` Hello? Hello? '' <newline> <newline> Jenny whispered she could sense his name coming from nowhere. A thin, sad looking dude in what appeared to be a complete punk charade was playing on him whilst he secured a massive candy bar with his initials. All around him were carnivals. In other words, Kim Kardashian and Veronica Reynolds at the ends of the job in Canada. Every one of them was right there. Half done. He had actually been invited back to a popular house party. A home party for a few years. One that made him happy enough to not be killed. He had done his best to regain his consciousness and own this carnival. This didn't make sense. <newline> <new\n"
          ]
        }
      ],
      "source": [
        "ids = tokenizer.encode('You ''ve finally managed to discover the secret to immortality . Suddenly , Death appears before you , hands you a business card , and says , `` When you realize living forever sucks , call this number , I ''ve got a job offer for you .',\n",
        "                      return_tensors='pt').to(device)\n",
        "\n",
        "story = model.generate(\n",
        "    ids,\n",
        "    do_sample=True,\n",
        "    max_new_tokens=150,\n",
        "    top_k=0,\n",
        "    num_return_sequences=1,\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(story[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiiWLAet-yu-",
        "outputId": "abb23d43-a9fb-45b6-83e0-0c0ebf14931c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "prompt :\n",
            "Through Iron And Flame\n",
            "story:\n",
            "The river Styx was thin, but still had the stark blue hue of pink and orange, burning with the sounds of the river Styx. The year was 1500, and everyone had accepted the charade of time from high above. Many stared bemused at the beast, and the gladty of the herd. Tycho Novgorville, the charge at the tracks was different, in many respects. <newline> <newline> Every centurion had come looking for the river Styx upon its skin, sealing their wounds for days. All around them were men in bays and ribbons, were weapons of traditional weaponry, and each assumed eye own. A gigantic castle was on its hind legs, projected to soar above the courtyard\n"
          ]
        }
      ],
      "source": [
        "ids = tokenizer.encode('Through Iron And Flame',\n",
        "                      return_tensors='pt').to(device)\n",
        "\n",
        "story = model.generate(\n",
        "    ids,\n",
        "    do_sample=True,\n",
        "    max_new_tokens=150,\n",
        "    top_k=0,\n",
        "    num_return_sequences=1,\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(story[0], skip_special_tokens=True))"
      ]
    }
  ]
}